# Contents of lora_config.yaml
# mlx-lm LoRA fine-tuning config for Bhagavad Gita QA
# Pass with: python -m mlx_lm.lora -c lora_config.yaml

model: "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
data: "/Users/sj/DevManus/gitaSLM/data"

# Training
train: true
fine_tune_type: "lora"    # | lora | Use `dora` for slightly better quality |
num_layers: 16            # | 16 | LoRA layers; increase for better results, uses more RAM |
batch_size: 2             # | 2 | Use 1 if you have 16GB RAM, 4 for 64GB | 
iters: 600                # | 600 | ~600–1000 is good for 500 examples |
learning_rate: 1e-4      # | 1e-4 | Standard starting point for LoRA |
mask_prompt: true          # ← KEY FIX: only train on answer tokens
seed: 42

# LoRA hyperparameters (must go here, not as CLI flags)
lora_parameters:
  rank: 8
  scale: 10.0              # stable default = ~1.25 * rank
  dropout: 0.05            # small dropout helps generalisation

# Checkpointing & eval
adapter_path: "./gita_adapters_v2"
save_every: 100
steps_per_eval: 100
steps_per_report: 10
val_batches: 25
max_seq_length: 2048

### Memory Guide by RAM
#| Mac RAM | Batch Size | Max Model Size |
#|---------|-----------|-----------------|
#| 16 GB   | 1         | 7B (4-bit)      |
#| 32 GB   | 2–4       | 7B–13B (4-bit)  |
#| 64 GB   | 4–8       | 30B+ (4-bit)    |
#| 96 GB+  | 8+        | 70B (4-bit)     |
